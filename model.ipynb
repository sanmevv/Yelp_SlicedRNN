{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Mtpn138LquF",
        "outputId": "98b827dd-d245-4c08-9e7b-72383f019f58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Tljk7IELiUL"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 512\n",
        "EMBEDDING_DIM = 200\n",
        "# this value is fixed since we are using pre-trained GloVe word embeddings\n",
        "GloVe_PATH = '/content/drive/MyDrive/NLP smtg/data/glove.6B.200d.txt'\n",
        "\n",
        "DATA_PATH = {\n",
        "    'Yelp2015': '/content/drive/MyDrive/NLP smtg/data/yelp_2015.csv'\n",
        "}\n",
        "\n",
        "TFRECORD_DIR = './tfrecord'\n",
        "import os\n",
        "MODEL_DIR = './saved_model'\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "BEST_MODEL_PATH = os.path.join(MODEL_DIR, 'best_model.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RFZZCt1PnRZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import pickle\n",
        "import argparse\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.keras.utils import generic_utils\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "from tensorflow.keras.layers import Embedding, Dense, GRU, Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noK0YXtqPsMe"
      },
      "outputs": [],
      "source": [
        "def _bytes_features(value):\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bk3EVO9yPvGX"
      },
      "outputs": [],
      "source": [
        "def _int64_features(value):\n",
        "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vz_QRcAoPz_M"
      },
      "outputs": [],
      "source": [
        "class TrainDataset:\n",
        "    def __init__(self, args):\n",
        "        self.batch_size = args.batch_size\n",
        "        self.train_record_path = os.path.join(TFRECORD_DIR, 'train.tfrecord')\n",
        "        self.val_record_path = os.path.join(TFRECORD_DIR, 'val.tfrecord')\n",
        "        self.test_record_path = os.path.join(TFRECORD_DIR, 'test.tfrecord')\n",
        "        self.tokenizer_path = os.path.join(MODEL_DIR, 'tokenizer.bin')\n",
        "        if args.create_tfrecord:\n",
        "            print('reading data')\n",
        "            if args.dataset == 'Custom':\n",
        "                assert os.path.isfile(args.train_data_path)\n",
        "                val_df, test_df, train_df = \\\n",
        "                    self.split_csv(args.train_data_path, [args.val_size, args.test_size, 1 - args.val_size - args.test_size])\n",
        "                train_data = train_df.to_numpy()\n",
        "                val_data = val_df.to_numpy()\n",
        "                test_data = test_df.to_numpy()\n",
        "            else:\n",
        "                '''\n",
        "                for Yelp dataset, only the train csv is provided. To make the process same as custom dataset,\n",
        "                we load the csv, split into train-val-test and save to './data/tmp'\n",
        "                '''\n",
        "                assert os.path.isfile(DATA_PATH[args.dataset])\n",
        "                val_df, test_df, train_df = \\\n",
        "                    self.split_csv(DATA_PATH[args.dataset], [args.val_size, args.test_size, 1 - args.val_size - args.test_size])\n",
        "                train_data = self.preprocess_yelp(train_df)\n",
        "                val_data = self.preprocess_yelp(val_df)\n",
        "                test_data = self.preprocess_yelp(test_df)\n",
        "            del train_df, val_df, test_df\n",
        "            print('     Done')\n",
        "            print('start fitting tokenizer...')\n",
        "            tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=args.max_num_words)\n",
        "            tokenizer.fit_on_texts(texts=train_data[:, 1])\n",
        "            print('     Done')\n",
        "            shutil.rmtree(TFRECORD_DIR, ignore_errors=True)\n",
        "            os.makedirs(TFRECORD_DIR, exist_ok=False)\n",
        "            self.create_tfrecord(train_data, self.train_record_path, tokenizer)\n",
        "            self.create_tfrecord(val_data, self.val_record_path, tokenizer)\n",
        "            self.create_tfrecord(test_data, self.test_record_path, tokenizer)\n",
        "            with open(self.tokenizer_path, 'wb') as f:\n",
        "                pickle.dump(tokenizer, f)\n",
        "            del train_data, val_data, test_data, tokenizer\n",
        "        self.record_path = {\n",
        "            'train': self.train_record_path,\n",
        "            'val': self.val_record_path,\n",
        "            'test': self.test_record_path\n",
        "        }\n",
        "        assert os.path.isfile(self.train_record_path) \\\n",
        "               and os.path.isfile(self.val_record_path) \\\n",
        "               and os.path.isfile(self.test_record_path) \\\n",
        "               and os.path.isfile(self.tokenizer_path)\n",
        "\n",
        "\n",
        "    def get_datasets(self, split='train'):\n",
        "        assert split in self.record_path\n",
        "\n",
        "        def decode_fn(example):\n",
        "            feature = tf.io.parse_single_example(\n",
        "                example,\n",
        "                features={\n",
        "                    'sequence': tf.io.FixedLenFeature([], dtype=tf.string),\n",
        "                    'label': tf.io.FixedLenFeature([], dtype=tf.int64),\n",
        "                }\n",
        "            )\n",
        "            sequence = tf.io.decode_raw(feature['sequence'], tf.int32)\n",
        "            return tf.reshape(sequence, (8, 8, 8)), feature['label']\n",
        "\n",
        "        dataset = tf.data.TFRecordDataset([self.record_path[split]]).map(decode_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(self.batch_size)\n",
        "        return dataset\n",
        "\n",
        "    def get_vocabulary(self):\n",
        "        with open(self.tokenizer_path, 'rb') as f:\n",
        "            tokenizer = pickle.load(f)\n",
        "        return tokenizer.word_index\n",
        "\n",
        "\n",
        "    def create_tfrecord(self, data, save_path, tokenizer):\n",
        "        with tf.io.TFRecordWriter(save_path) as writer:\n",
        "            label = data[:, 0].astype(np.int32)\n",
        "            text = data[:, 1]\n",
        "            text_ids = tokenizer.texts_to_sequences(text)\n",
        "            text_pad = tf.keras.preprocessing.sequence.pad_sequences(text_ids, maxlen=MAX_LEN).astype(np.int32)\n",
        "            for idx in range(label.shape[0]):\n",
        "                feature = {\n",
        "                    'sequence': _bytes_features(text_pad[idx].tobytes()),\n",
        "                    'label': _int64_features(label[idx]),\n",
        "                }\n",
        "                msg = tf.train.Example(features=tf.train.Features(feature=feature)).SerializeToString()\n",
        "                writer.write(msg)\n",
        "\n",
        "    def split_csv(self, csv_path, sizes):\n",
        "        assert (isinstance(sizes, list) or isinstance(sizes, tuple)) and np.isclose(np.sum(sizes), 1)\n",
        "        csv_data = pd.read_csv(csv_path, index_col=0)\n",
        "        # Yelp dataset contains index column but most custom data doesn't.\n",
        "        if csv_data.shape[1] == 1:\n",
        "            csv_data = pd.read_csv(csv_path, index_col=None)\n",
        "        s1 = csv_data.shape[0]\n",
        "        csv_data = csv_data.dropna()\n",
        "        if csv_data.shape[0] != s1:\n",
        "            print('drop {} nan samples'.format(s1 - csv_data.shape[0]))\n",
        "        csv_data = csv_data.sample(frac=1, random_state=0)\n",
        "        sizes = np.cumsum(sizes)\n",
        "        i0 = 0\n",
        "        dfs = []\n",
        "        for s in sizes:\n",
        "            i1 = int(s * csv_data.shape[0])\n",
        "            dfs.append(csv_data.iloc[i0:i1])\n",
        "            i0 = i1\n",
        "        return dfs\n",
        "\n",
        "    def preprocess_yelp(self, df):\n",
        "        # the class index of yelp dataset start from 1, we need to make it start from 0\n",
        "        label = df['stars'].to_numpy() - 1\n",
        "        text = df['text'].to_numpy()\n",
        "        data = np.concatenate([label[:, np.newaxis], text[:, np.newaxis]], axis=1)\n",
        "        return data\n",
        "\n",
        "\n",
        "class TestDataset:\n",
        "    def __init__(self, args):\n",
        "        assert os.path.isfile(args.data_path)\n",
        "        text = []\n",
        "        self.tokenizer_path = os.path.join(MODEL_DIR, 'tokenizer.bin')\n",
        "        with open(args.data_path) as f:\n",
        "            for line in f.readlines():\n",
        "                text.append(line)\n",
        "        with open(self.tokenizer_path, 'rb') as f:\n",
        "            tokenizer = pickle.load(f)\n",
        "        text_ids = tokenizer.texts_to_sequences(text)\n",
        "        self.text_ids_pad = tf.keras.preprocessing.sequence.pad_sequences(text_ids, maxlen=MAX_LEN).astype(np.int32)\n",
        "        self.text_ids_pad = self.text_ids_pad.reshape((-1, 8, 8, 8))\n",
        "\n",
        "    def get_text_sequence(self):\n",
        "        return self.text_ids_pad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0dpkWzbQUHf"
      },
      "outputs": [],
      "source": [
        "class ModelTimeDistributed(tf.keras.layers.Wrapper):\n",
        "    '''\n",
        "    Simplified module of tf.keras.layers.TimeDistributed for tf.keras.Model,\n",
        "    since in current Tensorflow version, the TimeDistributed Model has bug when computing output shape of tf.keras.Model or GRU module\n",
        "    '''\n",
        "    def __init__(self, layer, **kwargs):\n",
        "        if not isinstance(layer, tf.keras.Model):\n",
        "            raise ValueError(\n",
        "                'Please initialize `TimeDistributed` layer with a '\n",
        "                '`tf.keras.layers.Layer` instance. You passed: {input}'.format(\n",
        "                    input=layer))\n",
        "        super(ModelTimeDistributed, self).__init__(layer, **kwargs)\n",
        "\n",
        "    def _get_shape_tuple(self, init_tuple, tensor, start_idx, int_shape=None):\n",
        "        # replace all None in int_shape by K.shape\n",
        "        if int_shape is None:\n",
        "            int_shape = K.int_shape(tensor)[start_idx:]\n",
        "        if not any(not s for s in int_shape):\n",
        "            return init_tuple + tuple(int_shape)\n",
        "        shape = K.shape(tensor)\n",
        "        int_shape = list(int_shape)\n",
        "        for i, s in enumerate(int_shape):\n",
        "            if not s:\n",
        "                int_shape[i] = shape[start_idx + i]\n",
        "        return init_tuple + tuple(int_shape)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        input_shape = tensor_shape.TensorShape(input_shape).as_list()\n",
        "        child_input_shape = tensor_shape.TensorShape([input_shape[0]] +\n",
        "                                                     input_shape[2:])\n",
        "        # child_output_shape = self.layer.compute_output_shape(child_input_shape)\n",
        "        # if not isinstance(child_output_shape, tensor_shape.TensorShape):\n",
        "        #     child_output_shape = tensor_shape.TensorShape(child_output_shape)\n",
        "        # child_output_shape = child_output_shape.as_list()\n",
        "        timesteps = input_shape[1]\n",
        "        return tensor_shape.TensorShape([child_input_shape[0], timesteps] +\n",
        "                                        list(self.layer.output_shape[1:]))\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "        kwargs = {}\n",
        "        if generic_utils.has_arg(self.layer.call, 'training'):\n",
        "            kwargs['training'] = training\n",
        "\n",
        "        input_shape = K.int_shape(inputs)\n",
        "        input_length = input_shape[1]\n",
        "        if not input_length:\n",
        "            input_length = array_ops.shape(inputs)[1]\n",
        "        inner_input_shape = self._get_shape_tuple((-1,), inputs, 2)\n",
        "        # Shape: (num_samples * timesteps, ...). And track the\n",
        "        # transformation in self._input_map.\n",
        "        inputs = array_ops.reshape(inputs, inner_input_shape)\n",
        "        # (num_samples * timesteps, ...)\n",
        "        if generic_utils.has_arg(self.layer.call, 'mask') and mask is not None:\n",
        "            inner_mask_shape = self._get_shape_tuple((-1,), mask, 2)\n",
        "            kwargs['mask'] = K.reshape(mask, inner_mask_shape)\n",
        "\n",
        "        y = self.layer(inputs, **kwargs)\n",
        "\n",
        "        # Shape: (num_samples, timesteps, ...)\n",
        "        output_shape = self.compute_output_shape(input_shape).as_list()\n",
        "        output_shape = self._get_shape_tuple((-1, input_length), y, 1,\n",
        "                                             output_shape[2:])\n",
        "        y = array_ops.reshape(y, output_shape)\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KXTmaS4QtPs"
      },
      "outputs": [],
      "source": [
        "def create_model(args, vocab=None):\n",
        "    print(\"Loading GloVe embeddings\")\n",
        "    if not os.path.isfile(GloVe_PATH):\n",
        "        raise FileNotFoundError(\"Can not find glove.6B.200d.txt in ./data. Please download from \"\n",
        "                                \"https://nlp.stanford.edu/projects/glove/ and save to ./data\")\n",
        "\n",
        "    if vocab is not None:\n",
        "        embeddings_index = {}\n",
        "        with open(GloVe_PATH) as f:\n",
        "            for line in tqdm(f.readlines()):\n",
        "                values = line.split()\n",
        "                word = values[0]\n",
        "                coefs = np.asarray(values[1:], dtype='float32')\n",
        "                embeddings_index[word] = coefs\n",
        "        print('Finish Loading GloVe embeddings')\n",
        "\n",
        "        # use pre-trained GloVe word embeddings to initialize the embedding layer\n",
        "        embedding_matrix = np.random.random((args.max_num_words + 1, EMBEDDING_DIM))\n",
        "        for word, i in vocab.items():\n",
        "            if i < args.max_num_words:\n",
        "                embedding_vector = embeddings_index.get(word)\n",
        "                if embedding_vector is not None:\n",
        "                    # words not found in embedding index will be random initialized.\n",
        "                    embedding_matrix[i] = embedding_vector\n",
        "\n",
        "        embedding_layer = Embedding(args.max_num_words + 1,\n",
        "                                    EMBEDDING_DIM,\n",
        "                                    weights=[embedding_matrix],\n",
        "                                    input_length=MAX_LEN / 64,\n",
        "                                    trainable=True)\n",
        "    else:\n",
        "        embedding_layer = Embedding(args.max_num_words + 1,\n",
        "                                    EMBEDDING_DIM,\n",
        "                                    input_length=MAX_LEN / 64,\n",
        "                                    trainable=True)\n",
        "    if args.model == 'srnn':\n",
        "        # (-1, 8)\n",
        "        input1 = tf.keras.layers.Input(shape=(MAX_LEN // 64), dtype=tf.int32)\n",
        "        # (-1, 8, EMBEDDING_DIM)\n",
        "        embed1 = embedding_layer(input1)\n",
        "        # (-1, num_filters)\n",
        "        gru1 = tf.keras.layers.GRU(args.num_filters,\n",
        "                                   return_sequences=False,\n",
        "                                   activation=None,\n",
        "                                   recurrent_activation='sigmoid')(embed1)\n",
        "        encoder1 = tf.keras.Model(input1, gru1)\n",
        "        # (-1, 8, 8)\n",
        "        input2 = tf.keras.layers.Input(shape=(8, MAX_LEN // 64), dtype=tf.int32)\n",
        "        # (-1, 8, num_filters)\n",
        "        embed2 = ModelTimeDistributed(encoder1)(input2)\n",
        "        # (-1, num_filters)\n",
        "        gru2 = tf.keras.layers.GRU(args.num_filters,\n",
        "                                   return_sequences=False,\n",
        "                                   activation=None,\n",
        "                                   recurrent_activation='sigmoid')(embed2)\n",
        "        encoder2 = tf.keras.Model(input2, gru2)\n",
        "        # (-1, 8, 8, 8)\n",
        "        input3 = tf.keras.layers.Input(shape=(8, 8, MAX_LEN // 64), dtype=tf.int32)\n",
        "        # (-1, 8, num_filters)\n",
        "        embed3 = ModelTimeDistributed(encoder2)(input3)\n",
        "        # (-1, num_filters)\n",
        "        gru3 = tf.keras.layers.GRU(args.num_filters,\n",
        "                                   return_sequences=False,\n",
        "                                   activation=None,\n",
        "                                   recurrent_activation='sigmoid')(embed3)\n",
        "        # (-1, num_class)\n",
        "        pred = tf.keras.layers.Dense(args.num_class, activation='softmax', )(gru3)\n",
        "        model = tf.keras.Model(input3, pred)\n",
        "    else:\n",
        "        # (-1, 8, 8, 8)\n",
        "        inputs = tf.keras.layers.Input(shape=(8, 8, MAX_LEN // 64), dtype=tf.int32)\n",
        "        # (-1, MAX_LEN)\n",
        "        input_flatten = tf.reshape(inputs, (-1, MAX_LEN))\n",
        "        # (-1, MAX_LEN, EMBEDDING_DIM)\n",
        "        embed = embedding_layer(input_flatten)\n",
        "        # (-1, EMBEDDING_DIM)\n",
        "        gru = tf.keras.layers.GRU(args.num_filters,\n",
        "                                  return_sequences=False,\n",
        "                                  recurrent_activation='sigmoid',\n",
        "                                  activation=None)(embed)\n",
        "        # (-1, num_class)\n",
        "        pred = tf.keras.layers.Dense(args.num_class, activation='softmax')(gru)\n",
        "        model = tf.keras.Model(inputs, pred)\n",
        "        return model\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRiT3okJQzxp"
      },
      "outputs": [],
      "source": [
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjN_CnxQREe-",
        "outputId": "fed7b14d-b021-4ed0-f325-270f57061ee7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--num_filters'], dest='num_filters', nargs=None, const=None, default=50, type=<class 'int'>, choices=None, required=False, help='hidden size of the RNN', metavar=None)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data_path', type=str, help='path of the file containing the test sentences')\n",
        "parser.add_argument('--output_path', type=str, help='path to the file to save the output prediction')\n",
        "parser.add_argument('--model_path', type=str, help='')\n",
        "parser.add_argument('--batch_size', type=int, default=2048)\n",
        "parser.add_argument('--max_num_words', type=int, default=30000)\n",
        "parser.add_argument('--num_class', type=int, default=5)\n",
        "parser.add_argument('--num_filters', type=int, default=50, help='hidden size of the RNN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSSfXm9MRHS5"
      },
      "outputs": [],
      "source": [
        "def inference(args):\n",
        "    text_sequence = TestDataset(args).get_text_sequence()\n",
        "    model = create_model(args, None)\n",
        "    model.load_weights(BEST_MODEL_PATH)\n",
        "    pred = model.predict(text_sequence)\n",
        "    pred = np.argmax(pred, axis=-1) + 1\n",
        "    with open(args.output_path, 'w') as f:\n",
        "        for p in pred:\n",
        "            f.write('{}\\n'.format(p))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyXFHC-vRerV",
        "outputId": "3141587c-a3fc-49fd-990e-7817d7ae97a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--epochs'], dest='epochs', nargs=None, const=None, default=10, type=<class 'int'>, choices=None, required=False, help=None, metavar=None)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--create_tfrecord', action='store_true',\n",
        "                    help='''\n",
        "                    Should be only called once for every dataset.\n",
        "                    If set, the program would load data from csv specified by \"--dataset\", \"--train_data_path\" and\n",
        "                    \"--test_data_path\", create tfrecord and save to './tfrecord/train.tfrecord\",\n",
        "                    \"./tfrecord/val.tfrecord,\" and \"./tfrecord/test.tfrecord\".\n",
        "                    Otherwise, the program would ignore \"--dataset\", \"--train_data_path\" and \"--test_data_path\" and\n",
        "                    load directly from \"./tfrecord/train.tfrecord\", \"./tfrecord/val.tfrecord\", \"./tfrecord/test.tfrecord\".\n",
        "                    ''')\n",
        "parser.add_argument('--model', type=str, default='srnn', choices=['srnn', 'naive_rnn'])\n",
        "parser.add_argument('--dataset', type=str, default='Yelp2013',\n",
        "                    choices=['Yelp2013', 'Yelp2014', 'Yelp2015', 'Custom'], help='dataset to train on')\n",
        "parser.add_argument('--train_data_path', type=str, default='',\n",
        "                    help='if dataset is \"Custom\", please specify the path to the train csv')\n",
        "parser.add_argument('--test_data_path', type=str, default='',\n",
        "                    help='if dataset is \"Custom\", please specify the path to the test csv')\n",
        "parser.add_argument('--val_size', type=float, default=0.1, help='validation size')\n",
        "parser.add_argument('--test_size', type=float, default=0.1, help='test size if test_csv is not specified')\n",
        "parser.add_argument('--num_filters', type=int, default=50, help='hidden size of the RNN')\n",
        "parser.add_argument('--learning_rate', type=float, default=1e-3)\n",
        "parser.add_argument('--batch_size', type=int, default=2048)\n",
        "parser.add_argument('--max_num_words', type=int, default=30000)\n",
        "parser.add_argument('--num_class', type=int, default=5)\n",
        "parser.add_argument('--epochs', type=int, default=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLbAVMSNRjuq"
      },
      "outputs": [],
      "source": [
        "def train(args):\n",
        "    dataset = TrainDataset(args)\n",
        "    train_ds = dataset.get_datasets('train')\n",
        "    val_ds = dataset.get_datasets('val')\n",
        "    test_ds = dataset.get_datasets('test')\n",
        "    vocabulary = dataset.get_vocabulary()\n",
        "    del dataset\n",
        "    model_callback = tf.keras.callbacks.ModelCheckpoint(BEST_MODEL_PATH, save_best_only=True, save_weights_only=True,\n",
        "                                                        verbose=1, monitor='val_acc', mode='max')\n",
        "    model = create_model(args, vocabulary)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(args.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                  metrics=['acc'])\n",
        "    model.fit(train_ds, epochs=args.epochs, validation_data=val_ds, callbacks=[model_callback])\n",
        "    del model, train_ds, val_ds\n",
        "\n",
        "    best_model = create_model(args, None)\n",
        "    best_model.compile(optimizer=tf.keras.optimizers.Adam(args.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
        "                       loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                       metrics=['acc'])\n",
        "    best_model.load_weights(BEST_MODEL_PATH)\n",
        "    print(best_model.evaluate(test_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xU79KLvcS3At",
        "outputId": "a9466e62-0002-4dca-c81d-6cdd74457249"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reading data\n",
            "drop 1 nan samples\n",
            "     Done\n",
            "start fitting tokenizer...\n",
            "     Done\n",
            "Loading GloVe embeddings\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400000/400000 [00:16<00:00, 24294.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finish Loading GloVe embeddings\n",
            "Epoch 1/10\n",
            "    351/Unknown - 3693s 10s/step - loss: 0.8615 - acc: 0.6393\n",
            "Epoch 1: val_acc improved from -inf to 0.69065, saving model to ./saved_model/best_model.ckpt\n",
            "351/351 [==============================] - 3896s 11s/step - loss: 0.8615 - acc: 0.6393 - val_loss: 0.7302 - val_acc: 0.6907\n",
            "Epoch 2/10\n",
            "351/351 [==============================] - ETA: 0s - loss: 0.6872 - acc: 0.7071 \n",
            "Epoch 2: val_acc improved from 0.69065 to 0.70297, saving model to ./saved_model/best_model.ckpt\n",
            "351/351 [==============================] - 3761s 11s/step - loss: 0.6872 - acc: 0.7071 - val_loss: 0.6995 - val_acc: 0.7030\n",
            "Epoch 3/10\n",
            "351/351 [==============================] - ETA: 0s - loss: 0.6539 - acc: 0.7217 \n",
            "Epoch 3: val_acc improved from 0.70297 to 0.71209, saving model to ./saved_model/best_model.ckpt\n",
            "351/351 [==============================] - 4085s 12s/step - loss: 0.6539 - acc: 0.7217 - val_loss: 0.6806 - val_acc: 0.7121\n",
            "Epoch 4/10\n",
            "351/351 [==============================] - ETA: 0s - loss: 0.6289 - acc: 0.7332 \n",
            "Epoch 4: val_acc improved from 0.71209 to 0.71723, saving model to ./saved_model/best_model.ckpt\n",
            "351/351 [==============================] - 3773s 11s/step - loss: 0.6289 - acc: 0.7332 - val_loss: 0.6693 - val_acc: 0.7172\n",
            "Epoch 5/10\n",
            " 43/351 [==>...........................] - ETA: 51:33 - loss: 0.6139 - acc: 0.7406"
          ]
        }
      ],
      "source": [
        "args = parser.parse_args(['--dataset', 'Yelp2015', '--val_size', '0.1', '--test_size', '0.1', '--epochs', '10', '--create_tfrecord', '--batch_size', '2048'])\n",
        "train(args)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}